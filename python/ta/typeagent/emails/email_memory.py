# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

import os
from dataclasses import dataclass
import json
from pydantic.dataclasses import dataclass as pydantic_dataclass
import typechat
from ..aitools import utils
from ..knowpro import (
    secindex,
    convknowledge,
    search_query_schema,
    searchlang
)
from ..knowpro.convsettings import ConversationSettings
from ..knowpro.interfaces import (
    IConversation,
    IConversationSecondaryIndexes,
    IMessageCollection,
    ISemanticRefCollection,
    ITermToSemanticRefIndex,
    Term,
)
from ..storage.memory import semrefindex
from typeagent.storage.sqlite.provider import SqliteStorageProvider
        
from .email_message import EmailMessage

class EmailMemorySettings:
    def __init__(self, conversation_settings: ConversationSettings) -> None:
        self.language_model = convknowledge.create_typechat_model()
        self.query_translator = utils.create_translator(
            self.language_model, 
            search_query_schema.SearchQuery
        )
        self.conversation_settings = conversation_settings
        self.conversation_settings.semantic_ref_index_settings.auto_extract_knowledge = True


@dataclass
class EmailMemory(IConversation[EmailMessage, ITermToSemanticRefIndex]):
    settings: EmailMemorySettings
    name_tag: str
    messages: IMessageCollection[EmailMessage]
    semantic_refs: ISemanticRefCollection
    tags: list[str]
    semantic_ref_index: ITermToSemanticRefIndex
    secondary_indexes: IConversationSecondaryIndexes[EmailMessage] | None

    @classmethod
    async def create(
        cls,
        settings: ConversationSettings,
        name_tag: str | None = None,
        messages: IMessageCollection[EmailMessage] | None = None,
        semantic_refs: ISemanticRefCollection | None = None,
        semantic_ref_index: ITermToSemanticRefIndex | None = None,
        tags: list[str] | None = None,
        secondary_indexes: IConversationSecondaryIndexes[EmailMessage] | None = None,
    ) -> "EmailMemory":

        storage_provider = await settings.get_storage_provider()
        return cls(
            EmailMemorySettings(settings),
            name_tag or "",
            messages or await storage_provider.get_message_collection(),
            semantic_refs or await storage_provider.get_semantic_ref_collection(),
            tags if tags is not None else [],
            semantic_ref_index or await storage_provider.get_semantic_ref_index(),
            secondary_indexes
            or await secindex.ConversationSecondaryIndexes.create(
                storage_provider, settings.related_term_index_settings
            ),
        )

    # Add an email message to the memory.     
    async def add_message(self, message: EmailMessage) -> None:    
        await self.messages.append(message)
        self._commit()

    # Build an index using ALL messages in the memory
    async def build_index(
        self,
    ) -> None:
        await semrefindex.add_metadata_to_index(
            self.messages,
            self.semantic_refs,
            self.semantic_ref_index,
        )
        assert (
            self.settings is not None
        ), "Settings must be initialized before building index"

        await _add_synonyms_file_as_aliases(self, "emailVerbs.json")  
        self._commit()
        await semrefindex.build_semantic_ref(self, self.settings.conversation_settings)
        self._commit()
        await secindex.build_transient_secondary_indexes(self, self.settings.conversation_settings)
        self._commit()

    # Search email memory using language
    async def search_with_language(
            self,
            search_text: str,
            options: searchlang.LanguageSearchOptions | None = None,
            lang_search_filter: searchlang.LanguageSearchFilter | None = None,
            debug_context: searchlang.LanguageSearchDebugContext | None = None
        ) -> typechat.Result[list[searchlang.ConversationSearchResult]]:
        return await searchlang.search_conversation_with_language(
            self, 
            self.settings.query_translator,
            search_text,
            self._adjust_search_options(options),
            lang_search_filter,
            debug_context
        )
    
    @staticmethod
    def create_lang_search_options() -> searchlang.LanguageSearchOptions :
        return searchlang.LanguageSearchOptions(
            compile_options = searchlang.LanguageQueryCompileOptions(
                apply_scope=True,
                exact_scope=False, 
                verb_scope=True, 
                term_filter=None 
            ),
            exact_match=False,
            max_knowledge_matches=50,
            max_message_matches=25,
        )

    def _get_secondary_indexes(self) -> IConversationSecondaryIndexes[EmailMessage]:
        """Get secondary indexes, asserting they are initialized."""
        assert (
            self.secondary_indexes is not None
        ), "Use await f.create() to create an initialized instance"
        return self.secondary_indexes
    
    def _commit(self):
        provider = self.settings.conversation_settings.storage_provider
        if isinstance(provider, SqliteStorageProvider):
            provider.db.commit()

    def _adjust_search_options(self, options: searchlang.LanguageSearchOptions | None):
        # TODO Handle noise terms here
        if options is None:
            options = EmailMemory.create_lang_search_options()
        return options
    
# TODO: Migrate this to a shared API
async def _add_synonyms_file_as_aliases(conversation: IConversation, file_name: str) -> None:
    secondary_indexes = conversation.secondary_indexes
    assert secondary_indexes is not None
    assert secondary_indexes.term_to_related_terms_index is not None

    aliases = secondary_indexes.term_to_related_terms_index.aliases
    synonym_file = os.path.join(os.path.dirname(__file__), file_name)
    with open(synonym_file) as f:
        data: list[dict] = json.load(f)
    if data:
        for obj in data:
            text = obj.get("term")
            synonyms = obj.get("relatedTerms")
            if text and synonyms:
                related_term = Term(text=text.lower())
                for synonym in synonyms:
                    await aliases.add_related_term(synonym.lower(), related_term)
